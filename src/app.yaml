# Databricks App Configuration
# ============================
# Runtime configuration for the Flask app
#
# NOTE: For CI/CD deployments, this file is automatically regenerated with
# job IDs and secrets. For local deployment, follow the manual setup below.
#
# MANUAL SETUP (Local Deployment):
# ================================
# 1. Get your SQL Warehouse ID:
#    databricks warehouses list --output json | jq '.[].id'
#
# 2. Create a Lakebase (PostgreSQL) instance in your workspace:
#    Databricks workspace -> Catalog -> Lakebase -> Create
#
# 3. First deployment (creates jobs):
#    databricks bundle deploy -t dev --var sql_warehouse_id=<your-warehouse-id>
#
# 4. Get the job IDs created by the bundle:
#    databricks bundle summary -t dev --var sql_warehouse_id=<your-warehouse-id> --output json | \
#      jq '.resources.jobs | to_entries[] | {name: .key, id: .value.id}'
#
# 5. Update the values below with your configuration
#
# 6. Redeploy with updated configuration:
#    databricks bundle deploy -t dev --var sql_warehouse_id=<your-warehouse-id>

command:
  - gunicorn
  - --bind
  - 0.0.0.0:8000
  - --workers
  - "2"
  - --timeout
  - "300"
  - wsgi:app

env:
  # === REQUIRED: SQL Warehouse ID ===
  # Get it: databricks warehouses list --output json | jq '.[].id'
  - name: SQL_WAREHOUSE_ID
    value: ""

  # === REQUIRED: Job IDs (set after first deployment) ===
  # These are created by DAB on first deployment. Get them with:
  # databricks bundle summary -t dev --output json | jq '.resources.jobs'
  - name: DQ_GENERATION_JOB_ID
    value: ""

  - name: DQ_VALIDATION_JOB_ID
    value: ""

  # === REQUIRED: Lakebase PostgreSQL Configuration ===
  # Create a Lakebase instance in your workspace first
  - name: LAKEBASE_HOST
    value: ""

  - name: LAKEBASE_DATABASE
    value: "databricks_postgres"

  - name: LAKEBASE_PORT
    value: "5432"

  # === OPTIONAL: Model Serving Endpoint ===
  # Default: databricks-claude-sonnet-4-5
  # - name: MODEL_SERVING_ENDPOINT
  #   value: "databricks-claude-sonnet-4-5"
